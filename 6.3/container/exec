#!/usr/bin/env python3
import numpy as np
import tensorflow as tf
import random as rn

## Set up random seed and session to reproduce the results
np.random.seed(42)
rn.seed(42)
tf.set_random_seed(1111)

import sys
import time
from datetime import datetime
import json
import os
import boto3
import io
import tensorflow
from tensorflow import keras
from tensorflow.keras import models, layers, optimizers
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D
from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img
from tensorflow.keras.applications.inception_v3 import InceptionV3

print('Arguments received:', str(sys.argv))
print('Env variables',os.environ)

s3 = boto3.client('s3')
cloudwatch_client = boto3.client('cloudwatch', region_name='us-east-1')
s3_bucket = 'course-pdl-deep-learning-pipeline'
str_keras_model_path = '/app/flowerClassificationModel.h5'
str_tflite_model_path = '/app/flowerClassificationModel.tflite'

## Importing hyperparameters
result = s3.get_object(Bucket=s3_bucket, Key=sys.argv[1])
hyperparameters = json.loads(result["Body"].read().decode())
print(hyperparameters)

parameters = {}
parameters['input'] = hyperparameters

train_dir = '/app/flower-dataset/train'
validation_dir = '/app/flower-dataset/validation'
image_size = 224
train_batchsize = 50
val_batchsize = 10


## Importing model and setting up top layers
base_model = InceptionV3(weights='imagenet', include_top=False)
top_layer = base_model.output
top_layer = GlobalAveragePooling2D()(top_layer)
top_layer = Dense(1024, activation='relu')(top_layer)
predictions = Dense(3, activation='softmax')(top_layer)

model = Model(inputs=base_model.input, outputs=predictions)

## Freezing base layers
for layer in base_model.layers:
	layer.trainable = False


## Setting up augmentations and importing training and validation data
train_datagen = ImageDataGenerator(
		rescale=1./255,
		rotation_range=60,
		width_shift_range=0.2,
		height_shift_range=0.2,
		horizontal_flip=True,
		fill_mode='nearest')
train_generator = train_datagen.flow_from_directory(
		train_dir,
		target_size=(image_size, image_size),
		batch_size=train_batchsize,
		class_mode='categorical')

validation_datagen = ImageDataGenerator(rescale=1./255)
validation_generator = validation_datagen.flow_from_directory(
		validation_dir,
		target_size=(image_size, image_size),
		batch_size=val_batchsize,
		class_mode='categorical',
		shuffle=False)

model.compile(optimizer='rmsprop', loss='categorical_crossentropy')

# Train the model
history = model.fit_generator(
		train_generator,
		epochs=int(hyperparameters['num_of_epochs']),
		validation_data=validation_generator,
		validation_steps=validation_generator.samples/validation_generator.batch_size,
		verbose=1)

# Run the model on validation dataset to calculate accuracy
validation_generator = validation_datagen.flow_from_directory(
		validation_dir,
		target_size=(image_size, image_size),
		batch_size=val_batchsize,
		class_mode='categorical',
		shuffle=False)

predictions = model.predict_generator(validation_generator, verbose=1)
predicted_classes = np.argmax(predictions, axis=1)

errors = np.where(predicted_classes != validation_generator.classes)[0]
accuracy = 1 - ( len(errors) / validation_generator.samples )

print("Accuracy = {}".format(accuracy))
cloudwatch_client.put_metric_data(
	Namespace='production',
	MetricData=[
		{
			'MetricName' : 'Accuracy',
			'Dimensions' : [
				{
					'Name' : 'project',
					'Value' : 'training'
				}
			],
			'Timestamp': datetime.now(),
			'Value': accuracy
		}
	]
)


# Save the Keras model
model.save(str_keras_model_path)

# Convert the Keras model to TFLite model
converter = tensorflow.contrib.lite.TFLiteConverter.from_keras_model_file(
	str_keras_model_path, input_shapes={'input_1' : [1,299,299,3]}
)
tflite_model = converter.convert()
open(str_tflite_model_path, "wb").write(tflite_model)

# Upload the TFLite model to S3
with open(str_tflite_model_path, 'rb') as data:
	s3.upload_fileobj(data, s3_bucket, sys.argv[3])

parameters['output'] = {
	'accuracy' : accuracy,
	'model_path' : sys.argv[3]
}
print(parameters)
s3.upload_fileobj(io.BytesIO(json.dumps(parameters).encode('utf-8')), s3_bucket, sys.argv[2])

sys.exit(0)
